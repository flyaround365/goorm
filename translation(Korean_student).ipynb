{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Colab을 활용한 deep learning model inference\n",
        "\n",
        "*   transformer model\n",
        "*   BERT, GPT, 이미지 분류 및 인식 모델 등의 기반\n",
        "\n",
        "\n",
        "- https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/translation.ipynb"
      ],
      "metadata": {
        "id": "DYJQwQ2Ruq1-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWOVrrk8L1B_"
      },
      "outputs": [],
      "source": [
        "# Transformers 설치 방법\n",
        "! pip install transformers datasets evaluate sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fgQlUhhL1CB"
      },
      "source": [
        "# 번역[[translation]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zkXj606L1CC"
      },
      "source": [
        "번역은 한 언어로 된 시퀀스를 다른 언어로 변환합니다. 번역이나 요약은 입력을 받아 일련의 출력을 반환하는 강력한 프레임워크인 시퀀스-투-시퀀스 문제로 구성할 수 있는 대표적인 태스크입니다. 번역 시스템은 일반적으로 다른 언어로 된 텍스트 간의 번역에 사용되지만, 음성 간의 통역이나 텍스트-음성 또는 음성-텍스트와 같은 조합에도 사용될 수 있습니다.\n",
        "\n",
        "학습할 내용은:\n",
        "\n",
        "1. 영어 텍스트를 프랑스어로 번역하기 위해 [T5](https://huggingface.co/t5-small) 모델을 OPUS Books 데이터세트의 영어-프랑스어 하위 집합으로 파인튜닝하는 방법과\n",
        "2. 파인튜닝된 모델을 추론에 사용하는 방법입니다.\n",
        "\n",
        "<Tip>\n",
        "이 태스크 가이드는 아래 모델 아키텍처에도 응용할 수 있습니다.\n",
        "\n",
        "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
        "\n",
        "[BART](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/bart), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/bigbird_pegasus), [Blenderbot](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/blenderbot), [BlenderbotSmall](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/blenderbot-small), [Encoder decoder](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/encoder-decoder), [FairSeq Machine-Translation](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/fsmt), [GPTSAN-japanese](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/gptsan-japanese), [LED](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/led), [LongT5](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/longt5), [M2M100](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/m2m_100), [Marian](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/marian), [mBART](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/mbart), [MT5](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/mt5), [MVP](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/mvp), [NLLB](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/nllb), [NLLB-MOE](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/nllb-moe), [Pegasus](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/pegasus), [PEGASUS-X](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/pegasus_x), [PLBart](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/plbart), [ProphetNet](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/prophetnet), [SwitchTransformers](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/switch_transformers), [T5](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/t5), [XLM-ProphetNet](https://huggingface.co/docs/transformers/main/ko/tasks/../model_doc/xlm-prophetnet)\n",
        "\n",
        "<!--End of the generated tip-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HyTjL0LL1CD"
      },
      "source": [
        "## OPUS Books 데이터세트 가져오기[[load-opus-books-dataset]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0mKFktZL1CD"
      },
      "source": [
        "먼저 🤗 Datasets 라이브러리에서 [OPUS Books](https://huggingface.co/datasets/opus_books) 데이터세트의 영어-프랑스어 하위 집합을 가져오세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fonhYrl4L1CD"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "books = load_dataset(\"opus_books\", \"en-fr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fbJD73HL1CE"
      },
      "source": [
        "데이터세트를 `train_test_split` 메서드를 사용하여 훈련 및 테스트 데이터로 분할하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNAq_2d5L1CE"
      },
      "outputs": [],
      "source": [
        "books = books[\"train\"].train_test_split(test_size=0.001) # 0.2 -> 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbdGI_GpL1CE"
      },
      "source": [
        "훈련 데이터에서 예시를 살펴볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRTwEN_vL1CE"
      },
      "outputs": [],
      "source": [
        "books[\"train\"][123]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXzPghC4L1CE"
      },
      "source": [
        "반환된 딕셔너리의 `translation` 키가 텍스트의 영어, 프랑스어 버전을 포함하고 있는 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWt9ghjDL1CE"
      },
      "source": [
        "## 전처리[[preprocess]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWDCY-vaL1CF"
      },
      "source": [
        "다음 단계로 영어-프랑스어 쌍을 처리하기 위해 T5 토크나이저를 가져오세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fexZXNFtL1CF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaGFxGKuL1CF"
      },
      "source": [
        "만들 전처리 함수는 아래 요구사항을 충족해야 합니다:\n",
        "\n",
        "1. T5가 번역 태스크임을 인지할 수 있도록 입력 앞에 프롬프트를 추가하세요. 여러 NLP 태스크를 할 수 있는 모델 중 일부는 이렇게 태스크 프롬프트를 미리 줘야합니다.\n",
        "2. ~원어(영어)과 번역어(프랑스어)를 별도로 토큰화하세요. 영어 어휘로 사전 학습된 토크나이저로 프랑스어 텍스트를 토큰화할 수는 없기 때문입니다.~\n",
        "3. `max_length` 매개변수로 설정한 최대 길이보다 길지 않도록 시퀀스를 truncate하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khY8qB7pL1CF"
      },
      "outputs": [],
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"es\"\n",
        "prefix = \"translate English to French: \"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
        "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "truncation : max_length 128이상은 버림"
      ],
      "metadata": {
        "id": "-wgpC253ToYW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ORsEHzL1CF"
      },
      "source": [
        "전체 데이터세트에 전처리 함수를 적용하려면 🤗 Datasets의 `map` 메서드를 사용하세요. `map` 함수의 속도를 높이려면 `batched=True`를 설정하여 데이터세트의 여러 요소를 한 번에 처리하는 방법이 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6B8v4V2L1CF"
      },
      "outputs": [],
      "source": [
        "tokenized_books = books.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmIfq37SL1CF"
      },
      "source": [
        "이제 `DataCollatorForSeq2Seq`를 사용하여 학습용 배치를 생성합니다. 데이터세트의 최대 길이로 전부를 padding하는 대신, 데이터 정렬 중 각 배치의 최대 길이로 문장을 *동적으로 padding*하는 것이 더 효율적입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5ccBqRjL1CF"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBFARR3BL1CF"
      },
      "source": [
        "## 평가[[evalulate]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur7ZPAsbL1CF"
      },
      "source": [
        "훈련 중에 메트릭을 포함하면 모델의 성능을 평가하는 데 도움이 됩니다. 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) 라이브러리로 평가 방법(evaluation method)을 빠르게 가져올 수 있습니다. 현재 태스크에 적합한 SacreBLEU 메트릭을 가져오세요. (메트릭을 가져오고 계산하는 방법에 대해 자세히 알아보려면 🤗 Evaluate [둘러보기](https://huggingface.co/docs/evaluate/a_quick_tour)를 참조하세요):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUivy-YcL1CF"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDVjunulL1CF"
      },
      "source": [
        "SacreBLEU 점수를 계산하는 함수를 생성하세요.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgpl3aAzL1CF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJjQ6dSJL1CF"
      },
      "source": [
        "이제 `compute_metrics` 함수는 준비되었고, 훈련 과정을 설정할 때 다시 살펴볼 예정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfWVJw9AL1CG"
      },
      "source": [
        "## 훈련[[train]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_eI6kROL1CG"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "`Trainer`로 모델을 파인튜닝하는 방법에 익숙하지 않다면 [여기](https://huggingface.co/docs/transformers/main/ko/tasks/../training#train-with-pytorch-trainer)에서 기본 튜토리얼을 살펴보시기 바랍니다!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "모델을 훈련시킬 준비가 되었군요! `AutoModelForSeq2SeqLM`으로 T5를 로드하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LARZxbIL1CG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9glYSTRHL1CG"
      },
      "source": [
        "이제 세 단계만 거치면 끝입니다:\n",
        "\n",
        "1. `Seq2SeqTrainingArguments`에서 훈련 하이퍼파라미터를 정의하세요. 유일한 필수 매개변수는 모델을 저장할 위치인 `output_dir`입니다. 모델을 Hub에 푸시하기 위해 `push_to_hub=True`로 설정하세요. (모델을 업로드하려면 Hugging Face에 로그인해야 합니다.) `Trainer`는 에폭이 끝날때마다 SacreBLEU 메트릭을 평가하고 훈련 체크포인트를 저장합니다.\n",
        "2. `Seq2SeqTrainer`에 훈련 인수를 전달하세요. 모델, 데이터 세트, 토크나이저, data collator 및 `compute_metrics` 함수도 덩달아 전달해야 합니다.\n",
        "3. `train()`을 호출하여 모델을 파인튜닝하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8KRpXxnL1CG"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"my_awesome_opus_books_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=2,\n",
        "    max_steps=10,                       # gpu 계산량을 줄이기 위해서 추가\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_books[\"train\"],\n",
        "    eval_dataset=tokenized_books[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOElWTSeL1CG"
      },
      "source": [
        "## 추론[[inference]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36gny1iiL1CG"
      },
      "source": [
        "이제 모델을 파인튜닝했으니 추론에 사용할 수 있습니다!\n",
        "\n",
        "다른 언어로 번역하고 싶은 텍스트를 써보세요. T5의 경우 원하는 태스크를 입력의 접두사로 추가해야 합니다. 예를 들어 영어에서 프랑스어로 번역하는 경우, 아래와 같은 접두사가 추가됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43SWGEcWL1CG"
      },
      "outputs": [],
      "source": [
        "# text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n",
        "text = \"translate English to French: Would you like a cup of coffee?\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트를 토큰화하고 `input_ids`를 PyTorch 텐서로 반환하세요:  "
      ],
      "metadata": {
        "id": "FH9cZp9N2QnM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydfDuORqL1CK"
      },
      "outputs": [],
      "source": [
        "# inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "V9DJG9sl5Jw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensor : 자료형 중 하나 (list, set, tuple, dic... str, num)  \n",
        "행렬(matrix)을 n 차원 한거"
      ],
      "metadata": {
        "id": "FZrouaQ_szqb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxn-Wn_L1CK"
      },
      "source": [
        "`generate()` 메서드로 번역을 생성하세요. 다양한 텍스트 생성 전략 및 생성을 제어하기 위한 매개변수에 대한 자세한 내용은 [Text Generation](https://huggingface.co/docs/transformers/main/ko/tasks/../main_classes/text_generation) API를 살펴보시기 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SylTzmBAL1CK"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "Ses04sRK5cSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7GdeFz5L1CK"
      },
      "source": [
        "생성된 토큰 ID들을 다시 텍스트로 디코딩하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_5M11jpL1CK"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "0fgQlUhhL1CB",
        "-HyTjL0LL1CD",
        "xWt9ghjDL1CE",
        "EBFARR3BL1CF",
        "rfWVJw9AL1CG",
        "fOElWTSeL1CG"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}